{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3572cf",
   "metadata": {},
   "source": [
    "# Step15 원-핫 인코딩과 교차 엔트로피 오차\n",
    "\n",
    "2장은 목표값 t와 모델의 예측값 y 사이의 평균제곱오차(mse) 손실함수를 사용했다.(분류에서도 MSE를 사용가능 하긴 함)\n",
    "\n",
    "하지만 여기서는 분류에서 주로 사용하는 손실함수를 설명한다.\n",
    "1. 원-핫 인코딩\n",
    "2. 카탈로그 교차 엔트로피 오차(CCE)\n",
    "3. 이진 교차 엔트로피 오차(BCE)의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8c688",
   "metadata": {},
   "source": [
    "## 1. 원-핫 인코딩\n",
    "원-핫 인코딩은 분류에서 목표값 레이블을 표현하는 방법 중 하나로,\n",
    "\n",
    "각 카테고리에 대한 원-핫 인코딩은 부여받은 정수의 인덱스 위치만 1이고 나머지는 0을 갖는 것이다.\n",
    "\n",
    "<정수 카테고리에 대한 원-핫 인코딩 이진 행렬을 반환>\n",
    "```python\n",
    "#1\n",
    "tf.one_hot(indices, depth, ...) #indices의 정수를 depth 깊이로 원-핫 인코딩한 행렬의 텐서를 반환한다.\n",
    "#2\n",
    "tf.keras.utils.to_categorical(y, num_classes = None, dtype='float32')\n",
    "#정수의 클래스 레이블 벡터(y)를 클래스의 개수(num_classes)로 원-핫 인코딩한 넘파이 행렬을 반환한다.\n",
    "```\n",
    "<br><br>\n",
    "## step15_01\n",
    "### 원-핫 인코딩: tf.one_hot(), tf.keras.utils.to_categorical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20eabeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= [0 1 2 3 4 5 6 7 8 9]\n",
      "y1= [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "y2= [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "y = np.arange(10)\n",
    "print(\"y=\", y)\n",
    "\n",
    "y1 = tf.keras.utils.to_categorical(y)\n",
    "print(\"y1=\", y1)\n",
    "\n",
    "y2 = tf.one_hot(y, depth=10) #depth를 주어줘야한다.\n",
    "print(\"y2=\", y2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5240046d",
   "metadata": {},
   "source": [
    "## 2. 카테고리 교차 엔트로피 오차\n",
    "p.114!!-> [수식 15.1], [수식 15.2]\n",
    "\n",
    "카테고리 교차 엔트로피 함수는 여러 클래스 중에서 어느 하나의 클래스에 속지를 결정하는 다중 분류에 사용한다.\n",
    "\n",
    "출력층은 클래스의 개수만큼의 뉴런을 가지며, 출력층의 활성화 함수는 softmax를 사용하고, 출력 뉴런 중에서 **최대 예측 값을 갖는 뉴런에 대한 인덱스의 클래스로 분류**한다.\n",
    "<br><br>\n",
    "1. 카테고리 교차 엔트로피: 원-핫 인코딩 목표값\n",
    "```python\n",
    "CCE = tf.keras.losses.CategoricalCrossentropy()\n",
    "model.compile(optimizer='rmsprop', loss=CCE, metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "tf.keras.losses.CategoricalCrossentropy()는 다중 클래스 분류에서 **목표값이 원-핫 인코딩 되었을 때 교차 엔트로피 오차를 계산**한다.\n",
    "<br><br><br>\n",
    "2. 최소 카테고리 교차 엔트로피: 정수 레이블 인코딩\n",
    "```python\n",
    "CCE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer='rmsprop', loss=CCE, metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "tf.keras.losses.SparseCategoricalCrossentropy()는 다중 클래스 분류에서 **목표값이 정수 레이블로 인코딩 되었을 때 교차 엔트로피 오차를 계산**한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87097bc6",
   "metadata": {},
   "source": [
    "## step15_02\n",
    "### tf.keras.losses.CategoricalCrossentropy()\n",
    "1. 카테고리 교차 엔트로피: 원-핫 인코딩 목표값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d2efc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCE(t[i], y[0])\n",
      "CCE(t[0], y[0])= 0.916290731874155\n",
      "CCE(t[1], y[0])= 1.203972804325936\n",
      "CCE(t[2], y[0])= 1.6094379124341003\n",
      "CCE(t[3], y[0])= 2.3025850929940455\n",
      "CCE(t[i], y[1])\n",
      "CCE(t[0], y[1])= 2.3025850929940455\n",
      "CCE(t[1], y[1])= 1.203972804325936\n",
      "CCE(t[2], y[1])= 1.6094379124341003\n",
      "CCE(t[3], y[1])= 0.916290731874155\n",
      "CCE(np.vstack((t[1], t[1])), y)= 1.203972804325936\n",
      "[[0 1 0 0]\n",
      " [0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "CCE = tf.keras.losses.CategoricalCrossentropy()\n",
    "# t: 목표값, y: 원하는 출력값\n",
    "t= np.array([[1,   0,   0,   0],   #t[0]\n",
    "             [0,   1,   0,   0],   #t[1]\n",
    "             [0,   0,   1,   0],   #t[2]\n",
    "             [0,   0,   0,   1]])  #t[3]\n",
    "\n",
    "y =np.array([[0.4, 0.3, 0.2, 0.1], #y[0]\n",
    "             [0.1, 0.3, 0.2, 0.4]])#y[1]\n",
    "#1\n",
    "print(\"CCE(t[i], y[0])\")\n",
    "print(\"CCE(t[0], y[0])=\", CCE(t[0], y[0]).numpy() ) \n",
    "print(\"CCE(t[1], y[0])=\", CCE(t[1], y[0]).numpy() )\n",
    "print(\"CCE(t[2], y[0])=\", CCE(t[2], y[0]).numpy() )\n",
    "print(\"CCE(t[3], y[0])=\", CCE(t[3], y[0]).numpy() ) \n",
    "\n",
    "#2\n",
    "print(\"CCE(t[i], y[1])\")\n",
    "print(\"CCE(t[0], y[1])=\", CCE(t[0], y[1]).numpy() ) \n",
    "print(\"CCE(t[1], y[1])=\", CCE(t[1], y[1]).numpy() )\n",
    "print(\"CCE(t[2], y[1])=\", CCE(t[2], y[1]).numpy() )\n",
    "print(\"CCE(t[3], y[1])=\", CCE(t[3], y[1]).numpy() )\n",
    "\n",
    "#3\n",
    "print(\"CCE(np.vstack((t[1], t[1])), y)=\",\n",
    "       CCE(np.vstack((t[1], t[1])), y).numpy() )\n",
    "\n",
    "#이해 못하겠엉!\n",
    "print(np.vstack((t[1],t[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325fb3e7",
   "metadata": {},
   "source": [
    "1. y[0]은 t[0](CCE(t[0], y[0])),\n",
    "2. y[1]은 t[3](CCE(t[3], y[1]))과 교차엔트로피 오차가 제일 적다.\n",
    "\n",
    "-> 2개의 미니배치 출력 y의 목표값이 np.vstack(t[1], t[1])일 때 교차 엔트로피 오차는 1.20이다.\n",
    "\n",
    "즉, 교차 엔트로피의 평균 (CEE(y[0], t[0]) + CEE(y[1], t[1]))/2 이다.\n",
    "\n",
    "<br><br>\n",
    "-> y[0]은 t[0]과 오차가 가장 적고 y[1]은 t[3]과오차가 가작 작다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27f528",
   "metadata": {},
   "source": [
    "## step15_02\n",
    "### tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "2. 최소 카테고리 교차 엔트로피: 정수 레이블 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98ea7c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCE(t[i], y[0])\n",
      "SCE(t[0], y[0])= 0.91629076\n",
      "SCE(t[1], y[0])= 1.2039728\n",
      "SCE(t[2], y[0])= 1.609438\n",
      "SCE(t[3], y[0])= 2.3025851\n",
      "SCE(t[i], y[1])\n",
      "SCE(t[0], y[1])= 2.3025851\n",
      "SCE(t[1], y[1])= 1.2039728\n",
      "SCE(t[2], y[1])= 1.609438\n",
      "SCE(t[3], y[1])= 0.91629076\n",
      "SCE(tf.stack((t[1], t[1])), y)= 1.2039728\n",
      "1.4067054\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "SCE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "t = tf.convert_to_tensor([0, 1, 2, 3])\n",
    "y =tf.convert_to_tensor([[0.4, 0.3, 0.2, 0.1], #y[0]\n",
    "                         [0.1, 0.3, 0.2, 0.4]])#y[1]\n",
    "\n",
    "#1\n",
    "print(\"SCE(t[i], y[0])\")\n",
    "print(\"SCE(t[0], y[0])=\", SCE(t[0], y[0]).numpy() ) \n",
    "print(\"SCE(t[1], y[0])=\", SCE(t[1], y[0]).numpy() )\n",
    "print(\"SCE(t[2], y[0])=\", SCE(t[2], y[0]).numpy() )\n",
    "print(\"SCE(t[3], y[0])=\", SCE(t[3], y[0]).numpy() ) \n",
    "\n",
    "#2\n",
    "print(\"SCE(t[i], y[1])\")\n",
    "print(\"SCE(t[0], y[1])=\", SCE(t[0], y[1]).numpy() ) \n",
    "print(\"SCE(t[1], y[1])=\", SCE(t[1], y[1]).numpy() )\n",
    "print(\"SCE(t[2], y[1])=\", SCE(t[2], y[1]).numpy() )\n",
    "print(\"SCE(t[3], y[1])=\", SCE(t[3], y[1]).numpy() )\n",
    "\n",
    "#3\n",
    "print(\"SCE(tf.stack((t[1], t[1])), y)=\",\n",
    "       SCE(tf.stack((t[1], t[1])), y).numpy() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40743d5a",
   "metadata": {},
   "source": [
    "2개의 미니배치 출력 y의 목표값이 np.vstack((t[1],t[1]))일 때, 교차 엔트로피 오차는 1.20이다.\n",
    "\n",
    "즉, 교차 엔트로피의 평균'(SCE(t[1], y[0]) + SCE(t[1],y[1]))/2'이다.\n",
    "<br><br>\n",
    "\n",
    "-> y[0]은 t[0]과 오차가 가장 적고 y[1]은 t[3]과오차가 가작 작다.\n",
    "<br><br><br>\n",
    "\n",
    "## 3. 이진 교차 엔트로피 오차\n",
    "p. 117: 수식 참고!\n",
    "\n",
    "이진 교차 엔트로피 오차 손실함수는 출력층의 활성화 함수가 sigmoid를 사용할 때 주로 사용하며\n",
    "\n",
    "훈련 데이터의 목표값에 출력층의 레이블을 속함(1), 속하지 않음(0)으로 표현한다.\n",
    "<br><br>\n",
    "\n",
    "1. 출력층이 1뉴런(유닛)이면 1-레이블을 분류할 수 있다. 각 훈련 데이터의 목표값은 0 또는 1로 표현한다.\n",
    "2. 출력층이 2뉴런(유닛)이면 2-레이블을 분류할 수 있다. 각 훈련 데이터의 목표값은 [0,0], [0,1], [1,0], [1,1]로 4개의 클래스로 분류할 수 있다.\n",
    "\n",
    "여러 개의 뉴런(유닛)을 갖는 출력층으로 다중 레이블 분류를 할 수 있다. 출력층의 각 뉴런의 예측값이 1에 가까운지, 0에 가까운지에 따라 존재를 판단한다.\n",
    "```python\n",
    "BCE = tf.keras.losses.BinaryCrossentropy()\n",
    "model.compile(optimizer='rmsprop', loss=BCE, metrics=['accuracy'])\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy',metrics=['accruacy'])\n",
    "```\n",
    "tf.keras.losses.BinaryCrossentropy()는 다중 레이블 분류에서 이중 교차 엔트로피 오차를 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e587b",
   "metadata": {},
   "source": [
    "## step15_04\n",
    "### 이진 교차 엔트로피1: tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58d921b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE(t[i], y[0])\n",
      "BCE(t[0], y[0])= 0.4003672784541813\n",
      "BCE(t[1], y[0])= 0.5108254397382338\n",
      "BCE(t[2], y[0])= 0.6455745187904711\n",
      "BCE(t[3], y[0])= 0.8483069443724252\n",
      "BCE(t[i], y[1])\n",
      "BCE(t[0], y[1])= 0.8483069443724252\n",
      "BCE(t[1], y[1])= 0.5108254397382338\n",
      "BCE(t[2], y[1])= 0.6455745187904711\n",
      "BCE(t[3], y[1])= 0.40036727845418124\n",
      "BCE(np.vstack((t[0], t[0])), y)= 0.6243371114133032\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BCE = tf.keras.losses.BinaryCrossentropy()\n",
    "t= np.array([[1,   0,   0,   0],   #t[0]\n",
    "             [0,   1,   0,   0],   #t[1]\n",
    "             [0,   0,   1,   0],   #t[2]\n",
    "             [0,   0,   0,   1]])  #t[3]\n",
    "\n",
    "y =np.array([[0.4, 0.3, 0.2, 0.1], #y[0]\n",
    "             [0.1, 0.3, 0.2, 0.4]])#y[1]\n",
    "#1\n",
    "print(\"BCE(t[i], y[0])\")\n",
    "print(\"BCE(t[0], y[0])=\", BCE(t[0], y[0]).numpy() )\n",
    "print(\"BCE(t[1], y[0])=\", BCE(t[1], y[0]).numpy() )\n",
    "print(\"BCE(t[2], y[0])=\", BCE(t[2], y[0]).numpy() )\n",
    "print(\"BCE(t[3], y[0])=\", BCE(t[3], y[0]).numpy() ) \n",
    "\n",
    "#2\n",
    "print(\"BCE(t[i], y[1])\")\n",
    "print(\"BCE(t[0], y[1])=\", BCE(t[0], y[1]).numpy() ) \n",
    "print(\"BCE(t[1], y[1])=\", BCE(t[1], y[1]).numpy() )\n",
    "print(\"BCE(t[2], y[1])=\", BCE(t[2], y[1]).numpy() )\n",
    "print(\"BCE(t[3], y[1])=\", BCE(t[3], y[1]).numpy() )\n",
    "\n",
    "#3\n",
    "print(\"BCE(np.vstack((t[0], t[0])), y)=\",\n",
    "       BCE(np.vstack((t[0], t[0])), y).numpy() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6dd585",
   "metadata": {},
   "source": [
    "-> BCE(np.vstack((t[0], t[0])), y).numpy()가 0.62이다.\n",
    "\n",
    "즉, 이진 교차 엔트로피의 평균은 (BCE(t[0],y[0]) + BCE(t[0],y[1]))/2이다.\n",
    "<br><br>\n",
    "-> y[0]은 t[0]과 오차가 가장 적고 y[1]은 t[3]과오차가 가작 작다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4512ab8",
   "metadata": {},
   "source": [
    "## step15_05\n",
    "### 이진 교차 엔트로피2: tf.keras.losses.BinaryCrossentropy()\n",
    "step15_04 차이점 -> t가 이진 인코딩(원-핫 인코딩)이 아니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9325c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE(t[i], y[0])\n",
      "BCE(t[0], y[0])= 0.6121916959319459\n",
      "BCE(t[1], y[0])= 0.8573989362682357\n",
      "BCE(t[2], y[0])= 1.194880440902427\n",
      "BCE(t[3], y[0])= 1.0601313618501897\n",
      "BCE(t[i], y[1])\n",
      "BCE(t[0], y[1])= 1.0601313618501897\n",
      "BCE(t[1], y[1])= 0.8573989362682357\n",
      "BCE(t[2], y[1])= 0.7469407749841832\n",
      "BCE(t[3], y[1])= 0.6121916959319459\n",
      "BCE(np.vstack((t[0], t[0])), y)= 0.8361615288910678\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BCE = tf.keras.losses.BinaryCrossentropy()\n",
    "t= np.array([[1,   1,   0,   0],   #t[0]\n",
    "             [0,   1,   1,   0],   #t[1]\n",
    "             [0,   0,   1,   1],   #t[2]\n",
    "             [0,   1,   0,   1]])  #t[3]\n",
    "\n",
    "y =np.array([[0.4, 0.3, 0.2, 0.1], #y[0]\n",
    "             [0.1, 0.3, 0.2, 0.4]])#y[1]\n",
    "#1\n",
    "print(\"BCE(t[i], y[0])\")\n",
    "print(\"BCE(t[0], y[0])=\", BCE(t[0], y[0]).numpy() ) \n",
    "print(\"BCE(t[1], y[0])=\", BCE(t[1], y[0]).numpy() )\n",
    "print(\"BCE(t[2], y[0])=\", BCE(t[2], y[0]).numpy() )\n",
    "print(\"BCE(t[3], y[0])=\", BCE(t[3], y[0]).numpy() ) \n",
    "\n",
    "#2\n",
    "print(\"BCE(t[i], y[1])\")\n",
    "print(\"BCE(t[0], y[1])=\", BCE(t[0], y[1]).numpy() ) \n",
    "print(\"BCE(t[1], y[1])=\", BCE(t[1], y[1]).numpy() )\n",
    "print(\"BCE(t[2], y[1])=\", BCE(t[2], y[1]).numpy() )\n",
    "print(\"BCE(t[3], y[1])=\", BCE(t[3], y[1]).numpy() )\n",
    "\n",
    "#3\n",
    "print(\"BCE(np.vstack((t[0], t[0])), y)=\",\n",
    "       BCE(np.vstack((t[0], t[0])), y).numpy() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d6de2",
   "metadata": {},
   "source": [
    "# Step16 활성화 함수\n",
    "**p.122!!!**\n",
    "\n",
    "일반적으로 뉴런의 출력은 y = f(WX + b)와 같이 활성화 함수(f())에 의해 뉴런의 출력을 제어한다.\n",
    "\n",
    "<종류> \n",
    "1. linear: 입력 x를 그대로 출력한다.\n",
    "2. sigmoid: (0, 1) 범위의 값으로 변환한다.\n",
    "3. tanh: (-1, 1) 범위의 값으로 변환한다.\n",
    "4. relu: 양수는 그대로, 음수는 0으로 변환한다.\n",
    "5. LeakyReLU: 음수여도 alpha 값에 x를 곱해서 변환한다.\n",
    "6. softmax: 지수함수를 사용하여 입력 벡터 x를 확률로 변환한다.\n",
    "\n",
    "<사용법>\n",
    "\n",
    "완전 연결층을 생성하는 Dense 층을 생성할 때, activation 인수에 함수 이름 또는 문자열('linear', 'sigmoid', 'tanh', 'softmax' 등)로 활성화 함수를 지정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8c621",
   "metadata": {},
   "source": [
    "## step16_01\n",
    "### 활성화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a91c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1= [-10.  -1.   0.   1.  10.]\n",
      "y2= [4.5397868e-05 2.6894143e-01 5.0000000e-01 7.3105860e-01 9.9995458e-01]\n",
      "y3= [-1.        -0.7615942  0.         0.7615942  1.       ]\n",
      "y4= [ 0.  0.  0.  1. 10.]\n",
      "y5= [-1.  -0.1  0.   1.  10. ]\n",
      "y6= [[2.0607716e-09 1.6698603e-05 4.5391513e-05 1.2338691e-04 9.9981457e-01]]\n",
      "sum(y6)= 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x = tf.constant([-10, -1.0, 0.0, 1.0, 10], dtype = tf.float32)\n",
    "\n",
    "y1 = tf.keras.activations.linear(x) \n",
    "y2 = tf.keras.activations.sigmoid(x)\n",
    "y3 = tf.keras.activations.tanh(x)\n",
    "y4 = tf.keras.activations.relu(x)\n",
    "y5 = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "y6 = tf.keras.activations.softmax(tf.reshape(x, shape=(1, -1)))\n",
    "\n",
    "##linear = tf.keras.activations.get('linear')\n",
    "##y1 = linear(x)\n",
    "##\n",
    "##sigmoid = tf.keras.activations.get('sigmoid')\n",
    "##y2 = sigmoid(x)\n",
    "##\n",
    "##tanh = tf.keras.activations.get('tanh')\n",
    "##y3 = tanh(x)\n",
    "##\n",
    "##relu = tf.keras.activations.get('relu')\n",
    "##y4 = relu(x)\n",
    "##\n",
    "##y5 = relu(x, alpha=0.1) # LeakyReLU\n",
    "##softmax = tf.keras.activations.get('softmax')\n",
    "##y6 = softmax(tf.reshape(x, shape=(1, -1)))\n",
    "\n",
    "print(\"y1=\", y1.numpy())\n",
    "print(\"y2=\", y2.numpy())\n",
    "print(\"y3=\", y3.numpy())\n",
    "print(\"y4=\", y4.numpy())\n",
    "print(\"y5=\", y5.numpy())\n",
    "print(\"y6=\", y6.numpy())\n",
    "print(\"sum(y6)=\", np.sum(y6.numpy())) # 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14643772",
   "metadata": {},
   "source": [
    "# Step17 분류 성능평가\n",
    "**P.123**\n",
    "1. 정확도(accuracy): 전체에서 y_true, y_pred의 매칭 개수의 비율이다.\n",
    "\n",
    "TP+TN/TP+TN+FP+FN\n",
    "\n",
    "2. 정밀도(precision): Positive로 예측한 것 중에서 실제 Positive인 비율이다.\n",
    "\n",
    "TP/TP+FP\n",
    "\n",
    "3. 재현율(recall): 실제 Positive인 것 중에서 Positive로 예측한 비율이다.\n",
    "\n",
    "TP/FP+FN\n",
    "\n",
    "<br><br>\n",
    "다중 클래스 분류는 컨퓨전 행렬(C)에서 정확도, 정밀도, 재현율을 계산한다.\n",
    "\n",
    "이 단계에서는 텐서플로의 컨퓨전 행렬과 tf.keras.metrics의 성능 평가에 관해 설명한다.\n",
    "\n",
    "일반적으로 model.compile()의 학습환경 설정에서 metrics 인수에 모델의 평가방법을 리스트로 설정하면, model.fit()과 model.evaluate()에서 평가 방법에 따라 계산하여 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ee48d",
   "metadata": {},
   "source": [
    "## step17_01\n",
    "### 이진 분류: 정확도, 정밀도, 재현율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3352b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy1= tf.Tensor([0.33333334 0.33333334 1.         0.33333334 1.         1.        ], shape=(6,), dtype=float32)\n",
      "m.total=12.0, m.count=18.0\n",
      "accuracy2= 0.6666667\n",
      "y_true= [1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1]\n",
      "y_pred= [0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1]\n",
      "confusion_matrix(C)= tf.Tensor(\n",
      "[[9 3]\n",
      " [3 3]], shape=(2, 2), dtype=int32)\n",
      "m.total=12.0, m.count=18.0\n",
      "accuracy3= 0.6666667\n",
      "tp = 3.0\n",
      "tn= 9.0\n",
      "fp= 3.0\n",
      "fn= 3.0\n",
      "accuracy4 = 0.6666667\n",
      "precision = 0.5\n",
      "recall = 0.5\n",
      "f1 score = 0.5\n",
      "m.true_positives= [3.]\n",
      "m.false_positives [3.]\n",
      "precision= 0.5\n",
      "m.true_positives= [3.]\n",
      "m.false_negatives [3.]\n",
      "recall= 0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#1\n",
    "y_true = np.array([[1, 0, 0], #0\n",
    "                   [0, 1, 0], #1\n",
    "                   [0, 0, 1], #2\n",
    "                   [1, 0, 0], #0\n",
    "                   [0, 1, 0], #1\n",
    "                   [0, 0, 1]]);#2\n",
    "\n",
    "# binary: 1 above threshold=0.5, 0 below threshold= 0.5                           \n",
    "y_pred = np.array([[0.3, 0.6, 0.1], #1\n",
    "                   [0.6, 0.3, 0.1], #0\n",
    "                   [0.1, 0.3, 0.6], #2\n",
    "                   [0.3, 0.6, 0.1], #1\n",
    "                   [0.1, 0.6, 0.3], #1\n",
    "                   [0.3, 0.1, 0.6]]);#2\n",
    "#2\n",
    "accuracy1 =tf.keras.metrics.binary_accuracy(y_true, y_pred)\n",
    "print(\"accuracy1=\", accuracy1)\n",
    "\n",
    "#2-1\n",
    "m= tf.keras.metrics.BinaryAccuracy()\n",
    "m.update_state(y_true, y_pred)\n",
    "# m.total = tf.reduce_sum(accuracy1)\n",
    "# m.count = accuracy1.shape[0]\n",
    "accuracy2 = m.result() # m.total/m.count\n",
    "print(\"m.total={}, m.count={}\".format(m.total.numpy(), m.count.numpy()))\n",
    "print(\"accuracy2=\", accuracy2.numpy())\n",
    "\n",
    "#3: calculate confusion_matrix, C\n",
    "y_true = y_true.flatten()\n",
    "y_pred = np.cast['int'](y_pred.flatten()>0.5)\n",
    "\n",
    "##y_true= tf.reshape(y_true, [y_true.shape[0]*y_true.shape[1]] )\n",
    "##y_pred= tf.cast(y_pred>0.5, y_true.dtype)\n",
    "##y_pred= tf.reshape(y_pred,  shape= y_true.shape )\n",
    "\n",
    "##y_true= tf.keras.backend.flatten(y_true)\n",
    "##y_pred= tf.cast(y_pred>0.5, tf.int32)\n",
    "##y_pred= tf.keras.backend.flatten(y_pred)\n",
    "\n",
    "print(\"y_true=\",y_true)\n",
    "print(\"y_pred=\",y_pred)\n",
    "C = tf.math.confusion_matrix(y_true, y_pred)\n",
    "print(\"confusion_matrix(C)=\", C)\n",
    "\n",
    "#4:\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state(y_true, y_pred)\n",
    "print(\"m.total={}, m.count={}\".format(m.total.numpy(), m.count.numpy()))\n",
    "accuracy3 = m.result()  # m.total/m.count\n",
    "print(\"accuracy3=\", accuracy3.numpy())\n",
    "\n",
    "#5\n",
    "#5-1\n",
    "m = tf.keras.metrics.TruePositives()\n",
    "m.update_state(y_true, y_pred)\n",
    "tp = m.result()  # m.true_positives \n",
    "print(\"tp =\", tp.numpy())\n",
    "\n",
    "#5-2\n",
    "m = tf.keras.metrics.TrueNegatives()\n",
    "m.update_state(y_true, y_pred)\n",
    "tn = m.result() # m.accumulator[0] \n",
    "print(\"tn=\", tn.numpy())\n",
    "\n",
    "#5-3\n",
    "m = tf.keras.metrics.FalsePositives()\n",
    "m.update_state(y_true, y_pred)\n",
    "fp = m.result() # m.accumulator[0] sms\n",
    "print(\"fp=\", fp.numpy())\n",
    "\n",
    "#5-4\n",
    "m = tf.keras.metrics.FalseNegatives()\n",
    "m.update_state(y_true, y_pred)\n",
    "fn = m.result()# m.accumulator[0]  \n",
    "print(\"fn=\", fn.numpy())\n",
    "\n",
    "accuracy4  = (tp + tn)/(tp+tn+fp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall    = tp/(tp+fn)\n",
    "f1 = 2*tp/(2*tp + fp + fn) # harmonic mean of precision and recall\n",
    "print(\"accuracy4 =\", accuracy4.numpy())\n",
    "print(\"precision =\",precision.numpy())\n",
    "print(\"recall =\",   recall.numpy())\n",
    "print(\"f1 score =\", f1.numpy()) \n",
    "#6\n",
    "#6-1\n",
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state(y_true, y_pred)\n",
    "print(\"m.true_positives=\", m.true_positives.numpy())\n",
    "print(\"m.false_positives\", m.false_positives.numpy())\n",
    "print(\"precision=\", m.result().numpy())\n",
    "\n",
    "#6-2\n",
    "m = tf.keras.metrics.Recall()\n",
    "m.update_state(y_true, y_pred)\n",
    "print(\"m.true_positives=\", m.true_positives.numpy())\n",
    "print(\"m.false_negatives\", m.false_negatives.numpy())\n",
    "print(\"recall=\", m.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784acb61",
   "metadata": {},
   "source": [
    "## step17_02\n",
    "### 정밀도, 재현율: tf.keras.metrics.Precision(), tf.keras.metrics.Recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f94b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true1 [0 1 2 0 1 2]\n",
      "y_pred1 [1 0 2 1 1 2]\n",
      "confusion_matrics= tf.Tensor(\n",
      "[[0 2 0]\n",
      " [1 1 0]\n",
      " [0 0 2]], shape=(3, 3), dtype=int32)\n",
      "TOP_k tf.Tensor(\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [2 0]], shape=(6, 2), dtype=int32)\n",
      "In ech class, precision!\n",
      "p_0=0.3333333432674408, tp=[1.], fp=[2.]\n",
      "p_1=0.3333333432674408, tp=[1.], fp=[2.]\n",
      "p_2=0.3333333432674408, tp=[1.], fp=[2.]\n",
      "In each class, precision with top_k= 2\n",
      " p_0 =0.5, tp=[2.], fp= [2.]\n",
      " p_1 =0.4000000059604645, tp=[2.], fp= [3.]\n",
      " p_2 =0.6666666865348816, tp=[2.], fp= [1.]\n",
      "In each class, recall!\n",
      " recall_0 =0.0, tp=[0.], fn= [2.]\n",
      " recall_1 =0.5, tp=[1.], fn= [1.]\n",
      " recall_2 =1.0, tp=[2.], fn= [0.]\n",
      "In each class, recall with top_k= 2\n",
      " recall_0 =1.0, tp=[2.], fn= [0.]\n",
      " recall_1 =1.0, tp=[2.], fn= [0.]\n",
      " recall_2 =1.0, tp=[2.], fn= [0.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#1\n",
    "y_true = np.array([[1, 0, 0], #0\n",
    "                   [0, 1, 0], #1\n",
    "                   [0, 0, 1], #2\n",
    "                   [1, 0, 0], #0\n",
    "                   [0, 1, 0], #1\n",
    "                   [0, 0, 1]]);#2\n",
    "\n",
    "# binary: 1 above threshold=0.5, 0 below threshold= 0.5                           \n",
    "y_pred = np.array([[0.3, 0.6, 0.1], #1\n",
    "                   [0.6, 0.3, 0.1], #0\n",
    "                   [0.1, 0.3, 0.6], #2\n",
    "                   [0.3, 0.6, 0.1], #1\n",
    "                   [0.1, 0.6, 0.3], #1\n",
    "                   [0.3, 0.1, 0.6]]);#2\n",
    "num_class = y_true.shape[1]\n",
    "\n",
    "#2: C and TOP_k\n",
    "#2-1: y_pred의 레이블 번호를 계산하고, y_pred는 임계값 0.5를 적용, 예측 클래스 번호를 계산한다.\n",
    "#그리고 [그림17.3]의 컨퓨전 행렬 C를 계산한다. 임계값을 사용하는 #3-1, #4-1, [step17_1]의 #6에서 매칭을 설명할 수 있다.\n",
    "y_true1 = np.argmax(y_true, axis=1).flatten() #[0 1 2 0 1 2]\n",
    "y_pred1 = np.argmax(np.cast['int'](y_pred>0.5),axis=1).flatten() #[1 0 2 1 1 2]\n",
    "C = tf.math.confusion_matrix(y_true1, y_pred1)\n",
    "print('y_true1', y_true1)\n",
    "print('y_pred1', y_pred1)\n",
    "print('confusion_matrics=', C)\n",
    "\n",
    "#2-2: y_pred를 내림차순 정렬하고, k=2로 두번째 큰 값의 인덱스를 TOP_k에 저장한다. top_k[:,0]은 가장 큰 값의 인덱스이고\n",
    "# #2-1의 y_pred1과 같은 값이다. TOP_k[:,1]은 두번째 큰 값의 인덱스이다.\n",
    "k=2\n",
    "indx = tf.argsort(y_pred, axis=1, direction='DESCENDING')\n",
    "TOP_k = indx[:, :k]\n",
    "print(\"TOP_k\", TOP_k)\n",
    "\n",
    "#3\n",
    "print(\"In ech class, precision!\")\n",
    "#3-1: binary(1 above threshold=0.5, 0 below threshold=0.5)\n",
    "for i in range(num_class):\n",
    "    m = tf.keras.metrics.Precision(class_id=1)\n",
    "    m.update_state(y_true, y_pred)\n",
    "    tp = m.true_positives.numpy()\n",
    "    fp = m.false_positives.numpy()\n",
    "    p = m.result().numpy()\n",
    "    print('p_{}={}, tp={}, fp={}'.format(i,p,tp,fp))\n",
    "\n",
    "#3-2: the top-k classes with the highest predicted values\n",
    "print(\"In each class, precision with top_k=\", k)\n",
    "for i in range(num_class):\n",
    "    m = tf.keras.metrics.Precision(top_k=k, class_id = i)\n",
    "    m.update_state(y_true, y_pred)\n",
    "    tp = m.true_positives.numpy()\n",
    "    fp = m.false_positives.numpy()\n",
    "    p = m.result().numpy()\n",
    "    print(\" p_{} ={}, tp={}, fp= {}\".format(i,p, tp, fp))\n",
    "#4 \n",
    "print(\"In each class, recall!\")\n",
    "#4-1: binary(1 above threshold=0.5, 0 below threshold= 0.5)\n",
    "for i in range(num_class):\n",
    "    m = tf.keras.metrics.Recall(class_id = i)\n",
    "    m.update_state(y_true, y_pred)\n",
    "    tp = m.true_positives.numpy()\n",
    "    fn = m.false_negatives.numpy()\n",
    "    r = m.result().numpy()\n",
    "    print(\" recall_{} ={}, tp={}, fn= {}\".format(i,r, tp, fn))\n",
    "\n",
    "#4-2: the top-k classes with the highest predicted values\n",
    "print(\"In each class, recall with top_k=\", k)\n",
    "for i in range(num_class):\n",
    "    m = tf.keras.metrics.Recall(top_k=k, class_id = i)\n",
    "    m.update_state(y_true, y_pred)\n",
    "    r = m.result().numpy()\n",
    "    print(\" recall_{} ={}, tp={}, fn= {}\".format(i,r, tp, fn))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
